{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Gregor Pfalz\\ngithub: GPawi\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Author: Gregor Pfalz\n",
    "github: GPawi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import os\n",
    "#import sys\n",
    "import sqlalchemy\n",
    "import getpass\n",
    "#import datetime\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "from xlrd import XLRDError\n",
    "import pdb; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class connection_db(object):\n",
    "    def __init__(self, core, db = None, password = None, force_upload = False):\n",
    "        self.__core = core\n",
    "        self.__force_upload = force_upload\n",
    "        self.__coreid = core._data_preparation__coreid\n",
    "        if db is not None and password is not None:\n",
    "            self.__db = db\n",
    "            self.__password = password\n",
    "        elif db is None and password is None:\n",
    "            self.__db = input(f'What is the database name in which {self.__coreid} should be inserted? ')\n",
    "            self.__password = getpass.getpass(prompt='What is the password for that database? ')\n",
    "        elif db is not None and password is None:\n",
    "            self.__db = db\n",
    "            self.__password = getpass.getpass(prompt='What is the password for that database? ')\n",
    "        else:\n",
    "            self.__db = input(f'What is the database name in which {self.__coreid} should be inserted? ')\n",
    "            self.__password = password\n",
    "            \n",
    "        self.__engine = sqlalchemy.create_engine(f'postgresql://postgres:{self.__password}@localhost/{self.__db}')\n",
    "        \n",
    "        \n",
    "    def __upload_scientist__(self):\n",
    "        __core = self.__core\n",
    "        __engine = self.__engine\n",
    "        self.__scientist = __core._data_preparation__scientist\n",
    "        self.__scientist_columns = __core._data_preparation__scientist_columns\n",
    "        try:\n",
    "            __con = __engine.connect()\n",
    "            self.__scientist_duplicate_check = pd.merge(self.__scientist, pd.read_sql('scientist', __con), how ='inner', on = self.__scientist_columns)\n",
    "            #\n",
    "            if any(self.__scientist_duplicate_check.columns == 'scientistid') and (len(self.__scientist_duplicate_check) == len(self.__scientist)):\n",
    "                self.__scientist = self.__scientist_duplicate_check\n",
    "                __con.close()\n",
    "                print ('Scientist(s) already exist!')\n",
    "            elif any(self.__scientist_duplicate_check.columns == 'scientistid') and (len(self.__scientist_duplicate_check) != len(self.__scientist)) and (len(self.__scientist_duplicate_check)>0):\n",
    "                self.__new_scientist = self.__scientist[~self.__scientist.isin(self.__scientist_duplicate_check)].dropna()\n",
    "                self.__new_scientist.to_sql('scientist', __con, if_exists='append', index = False)\n",
    "                self.__scientist_duplicate_check = pd.merge(self.__scientist, pd.read_sql('scientist', __con), how ='inner', on = self.__scientist_columns)\n",
    "                self.__scientist = self.__scientist_duplicate_check\n",
    "                __con.close()\n",
    "                print ('Added new scientist(s)!')\n",
    "            else:\n",
    "                self.__scientist.to_sql('scientist', __con, if_exists='append', index = False)\n",
    "                self.__scientist_duplicate_check = pd.merge(self.__scientist, pd.read_sql('scientist', __con), how ='inner', on = self.__scientist_columns)\n",
    "                self.__scientist = self.__scientist_duplicate_check\n",
    "                __con.close()\n",
    "                print ('All scientist(s) added!')\n",
    "        except:\n",
    "            __con.close()\n",
    "            print ('There was an issue - Please report to Gregor Pfalz (Gregor.Pfalz@awi.de)!')\n",
    "                        \n",
    "    def __upload_expedition__(self):\n",
    "        __core = self.__core\n",
    "        __engine = self.__engine\n",
    "        __scientist = self.__scientist\n",
    "        self.__scientistid = __scientist['scientistid']\n",
    "        self.__expedition = __core._data_preparation__expedition\n",
    "        self.__expedition_columns = __core._data_preparation__expedition_columns\n",
    "        try:\n",
    "            __con = __engine.connect()\n",
    "            self.__expedition_duplicate_check = pd.merge(self.__expedition, pd.read_sql('expedition', __con), how ='inner', on = self.__expedition_columns)\n",
    "            if len(self.__expedition_duplicate_check) > 0:\n",
    "                self.__expedition = self.__expedition_duplicate_check\n",
    "                __con.close()\n",
    "                print ('Expedition already exists!')\n",
    "            else:\n",
    "                self.__expedition['scientistid'] = self.__scientistid\n",
    "                self.__expedition.to_sql('expedition', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print ('New expedition added!')\n",
    "        except:\n",
    "            __con.close()\n",
    "            print ('There was an issue - Please report to Gregor Pfalz (Gregor.Pfalz@awi.de)!')\n",
    "                        \n",
    "    def __upload_lake__(self):\n",
    "        __core = self.__core\n",
    "        __engine = self.__engine\n",
    "        self.__lake = __core._data_preparation__lake\n",
    "        self.__lake_columns = __core._data_preparation__lake_columns\n",
    "        try:\n",
    "            __con = __engine.connect()\n",
    "            self.__lake_duplicate_check = pd.merge(self.__lake, pd.read_sql('lake', __con), how ='inner', on = self.__lake_columns)\n",
    "            if len(self.__lake_duplicate_check) > 0:\n",
    "                self.__lake = self.__lake_duplicate_check\n",
    "                __con.close()\n",
    "                print ('Lake already exists!')\n",
    "            else:\n",
    "                self.__lake.to_sql('lake', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print ('New lake added!')\n",
    "        except:\n",
    "            __con.close()\n",
    "            print ('There was an issue - Please report to Gregor Pfalz (Gregor.Pfalz@awi.de)!')\n",
    "    \n",
    "    def __upload_drilling__(self):\n",
    "        __core = self.__core\n",
    "        __engine = self.__engine\n",
    "        __coreid = self.__coreid\n",
    "        __force_upload = self.__force_upload\n",
    "        self.__drilling = __core._data_preparation__drilling\n",
    "        self.__drilling_columns = __core._data_preparation__drilling_columns\n",
    "        try:\n",
    "            __con = __engine.connect()\n",
    "            self.__drilling_duplicate_check = pd.merge(self.__drilling, pd.read_sql('drilling', __con), how ='inner', on = self.__drilling_columns)\n",
    "            if len(self.__drilling_duplicate_check) > 0:\n",
    "                self.__drilling = self.__drilling_duplicate_check\n",
    "                __con.close()\n",
    "                if __force_upload == False:\n",
    "                    while True:\n",
    "                        self.__upload_query = input(f'Core {__coreid} already exist - Do you still want to continue uploading the data? Y/N? ')\n",
    "                        self.__upload_answer = self.__upload_query[0].lower() \n",
    "                        if self.__upload_query == '' or not self.__upload_answer in ['y','n']:\n",
    "                            print('Please answer with yes or no!') \n",
    "                        else:\n",
    "                            break\n",
    "                    if self.__upload_answer == 'y':\n",
    "                        self.__upload_stop = False\n",
    "                        print ('Ok!')\n",
    "                    if self.__upload_answer == 'n':\n",
    "                        raise Exception('Manually stopped upload process.')\n",
    "                else:\n",
    "                    self.__upload_stop = False\n",
    "                    print (f'Core {__coreid} already exists!')\n",
    "                \n",
    "            else:\n",
    "                self.__drilling.to_sql('drilling', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                self.__upload_stop = False\n",
    "                print ('New core information added!')\n",
    "        \n",
    "        except Exception:\n",
    "            __con.close()\n",
    "            self.__upload_stop = True\n",
    "            print ('Manually stopped upload process.')\n",
    "        \n",
    "        except:\n",
    "            __con.close()\n",
    "            print ('There was an issue - Please report to Gregor Pfalz (Gregor.Pfalz@awi.de)!')\n",
    "    \n",
    "    def __upload_participant__(self):\n",
    "        self.__participant = self.__scientist\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        self.__participant['coreid'] = __coreid\n",
    "        self.__participant = self.__participant[['scientistid','coreid']]\n",
    "        try:\n",
    "            __con = __engine.connect()\n",
    "            self.__participant_duplicate_check = pd.merge(self.__participant, pd.read_sql('participant', __con), how ='inner', on = ['scientistid','coreid'])\n",
    "            if (len(self.__participant_duplicate_check) == len(self.__participant)):\n",
    "                self.__participant = self.__participant_duplicate_check\n",
    "                __con.close()\n",
    "                print ('Participant(s) already registered!')\n",
    "            elif (len(self.__participant_duplicate_check) != len(self.__participant)):\n",
    "                self.__new_participant = self.__participant[~self.__participant.isin(self.__participant_duplicate_check)].dropna()\n",
    "                self.__new_participant.to_sql('participant', __con, if_exists='append', index = False)\n",
    "                self.__participant_duplicate_check = pd.merge(self.__participant, pd.read_sql('participant', __con), how ='inner', on = ['scientistid','coreid'])\n",
    "                self.__participant = self.__participant_duplicate_check\n",
    "                __con.close()\n",
    "                print (f'Added new participant(s) to {__coreid}!')\n",
    "            else:\n",
    "                self.__participant.to_sql('participant', __con, if_exists='append', index = False)\n",
    "                self.__participant_duplicate_check = pd.merge(self.__participant, pd.read_sql('participant', __con), how ='inner', on = ['scientistid','coreid'])\n",
    "                self.__participant = self.__participant_duplicate_check\n",
    "                __con.close()\n",
    "                print (f'Participant(s) added for {__coreid}!')\n",
    "        except:\n",
    "            __con.close()\n",
    "            print ('There was an issue - Please report to Gregor Pfalz (Gregor.Pfalz@awi.de)!')\n",
    "    \n",
    "    def __upload_organic__(self): \n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_organic  = __core._data_preparation__input_organic\n",
    "            self.__measurementids_organic = __core._data_preparation__input_organic.copy()\n",
    "            self.__measurementids_organic[['coreid','compositedepth']] = self.__measurementids_organic['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_organic = self.__measurementids_organic[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_organic['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            #self.__measurementids_organic = self.__measurementids_organic[~(self.__measurementids_organic['compositedepth'].str.contains(pat = 'duplicate')== True)]\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__measurement_organic_duplicate_check = pd.merge(self.__measurementids_organic, pd.read_sql('measurement', __con), how ='inner', on = ['measurementid', 'coreid'])\n",
    "                self.__measurement_organic_duplicate_check = self.__measurement_organic_duplicate_check.drop(columns = 'compositedepth_y')\n",
    "                self.__measurement_organic_duplicate_check = self.__measurement_organic_duplicate_check.rename(columns = {'compositedepth_x':'compositedepth'})\n",
    "                self.__measurementids_organic = self.__measurementids_organic.append(self.__measurement_organic_duplicate_check).drop_duplicates(keep=False)\n",
    "                self.__measurementids_organic.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for organic proxy')\n",
    "            finally:\n",
    "                self.__input_organic.to_sql('organic', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for organic proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that organic data were already uploaded.')\n",
    "    \n",
    "    def __upload_grainsize__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_grainsize  = __core._data_preparation__input_grainsize\n",
    "            self.__measurementids_grainsize = __core._data_preparation__input_grainsize.copy()\n",
    "            self.__measurementids_grainsize[['coreid','compositedepth']] = self.__measurementids_grainsize['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_grainsize = self.__measurementids_grainsize[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_grainsize['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__measurement_grainsize_duplicate_check = pd.merge(self.__measurementids_grainsize, pd.read_sql('measurement', __con), how ='inner', on = ['measurementid', 'coreid'])\n",
    "                self.__measurement_grainsize_duplicate_check = self.__measurement_grainsize_duplicate_check.drop(columns = 'compositedepth_y')\n",
    "                self.__measurement_grainsize_duplicate_check = self.__measurement_grainsize_duplicate_check.rename(columns = {'compositedepth_x':'compositedepth'})\n",
    "                self.__measurementids_grainsize = self.__measurementids_grainsize.append(self.__measurement_grainsize_duplicate_check).drop_duplicates(keep=False)\n",
    "                self.__measurementids_grainsize.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for grain size proxy')\n",
    "            finally:\n",
    "                self.__input_grainsize.to_sql('grainsize', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for grain size proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that grain size data were already uploaded.')        \n",
    "    \n",
    "    def __upload_element__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_element  = __core._data_preparation__input_element\n",
    "            self.__measurementids_element = __core._data_preparation__input_element.copy()\n",
    "            self.__measurementids_element[['coreid','compositedepth']] = self.__measurementids_element['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_element = self.__measurementids_element[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_element['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_element = self.__measurementids_element.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_ele = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_ele = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_element_duplicate_check = self.__measurementids_element.append(self.__down_measurement_ele)\n",
    "                self.__measurement_element_duplicates = self.__measurement_element_duplicate_check[self.__measurement_element_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_element_duplicate_free = self.__measurementids_element.append(self.__measurement_element_duplicates)\n",
    "                self.__measurement_element_duplicate_free = self.__measurement_element_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_element_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for element proxy')\n",
    "            finally:\n",
    "                self.__input_element.to_sql('element', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for element proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that element data were already uploaded.')\n",
    "            \n",
    "    \n",
    "    def __upload_mineral__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_mineral  = __core._data_preparation__input_mineral\n",
    "            self.__measurementids_mineral = __core._data_preparation__input_mineral.copy()\n",
    "            self.__measurementids_mineral[['coreid','compositedepth']] = self.__measurementids_mineral['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_mineral = self.__measurementids_mineral[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_mineral['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_mineral = self.__measurementids_mineral.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_mineral = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_mineral = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_mineral_duplicate_check = self.__measurementids_mineral.append(self.__down_measurement_mineral)\n",
    "                self.__measurement_mineral_duplicates = self.__measurement_mineral_duplicate_check[self.__measurement_mineral_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_mineral_duplicate_free = self.__measurementids_mineral.append(self.__measurement_mineral_duplicates)\n",
    "                self.__measurement_mineral_duplicate_free = self.__measurement_mineral_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_mineral_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for mineral proxy')\n",
    "            finally:\n",
    "                self.__input_mineral.to_sql('mineral', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for mineral proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that mineral data were already uploaded.')\n",
    "\n",
    "    \n",
    "    def __upload_diatom__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_diatom  = __core._data_preparation__input_diatom\n",
    "            self.__measurementids_diatom = __core._data_preparation__input_diatom.copy()\n",
    "            self.__measurementids_diatom[['coreid','compositedepth']] = self.__measurementids_diatom['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_diatom = self.__measurementids_diatom[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_diatom['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_diatom = self.__measurementids_diatom.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_diatom = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_diatom = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_diatom_duplicate_check = self.__measurementids_diatom.append(self.__down_measurement_diatom)\n",
    "                self.__measurement_diatom_duplicates = self.__measurement_diatom_duplicate_check[self.__measurement_diatom_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_diatom_duplicate_free = self.__measurementids_diatom.append(self.__measurement_diatom_duplicates)\n",
    "                self.__measurement_diatom_duplicate_free = self.__measurement_diatom_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_diatom_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for diatom proxy')\n",
    "            finally:\n",
    "                self.__input_diatom.to_sql('diatom', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for diatom proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that diatom data were already uploaded.')\n",
    "\n",
    "    \n",
    "    def __upload_chironomid__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_chironomid  = __core._data_preparation__input_chironomid\n",
    "            self.__measurementids_chironomid = __core._data_preparation__input_chironomid.copy()\n",
    "            self.__measurementids_chironomid[['coreid','compositedepth']] = self.__measurementids_chironomid['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_chironomid = self.__measurementids_chironomid[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_chironomid['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_chironomid = self.__measurementids_chironomid.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_chironomid = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_chironomid = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_chironomid_duplicate_check = self.__measurementids_chironomid.append(self.__down_measurement_chironomid)\n",
    "                self.__measurement_chironomid_duplicates = self.__measurement_chironomid_duplicate_check[self.__measurement_chironomid_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_chironomid_duplicate_free = self.__measurementids_chironomid.append(self.__measurement_chironomid_duplicates)\n",
    "                self.__measurement_chironomid_duplicate_free = self.__measurement_chironomid_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_chironomid_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for chironomid proxy')\n",
    "            finally:\n",
    "                self.__input_chironomid.to_sql('chironomid', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for chironomid proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that chironomid data were already uploaded.')\n",
    "\n",
    "    \n",
    "    def __upload_pollen__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_pollen  = __core._data_preparation__input_pollen\n",
    "            self.__measurementids_pollen = __core._data_preparation__input_pollen.copy()\n",
    "            self.__measurementids_pollen[['coreid','compositedepth']] = self.__measurementids_pollen['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_pollen = self.__measurementids_pollen[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_pollen['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_pollen = self.__measurementids_pollen.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_pollen = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_pollen = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_pollen_duplicate_check = self.__measurementids_pollen.append(self.__down_measurement_pollen)\n",
    "                self.__measurement_pollen_duplicates = self.__measurement_pollen_duplicate_check[self.__measurement_pollen_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_pollen_duplicate_free = self.__measurementids_pollen.append(self.__measurement_pollen_duplicates)\n",
    "                self.__measurement_pollen_duplicate_free = self.__measurement_pollen_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_pollen_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for pollen proxy')\n",
    "            finally:\n",
    "                self.__input_pollen.to_sql('pollen', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for pollen proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that pollen data were already uploaded.')\n",
    "\n",
    "    \n",
    "    def __upload_age__(self):\n",
    "        __core = self.__core\n",
    "        __coreid = self.__coreid\n",
    "        __engine = self.__engine\n",
    "        try:\n",
    "            self.__input_agedetermination  = __core._data_preparation__input_age\n",
    "            self.__measurementids_agedetermination = __core._data_preparation__input_age.copy()\n",
    "            self.__measurementids_agedetermination[['coreid','compositedepth']] = self.__measurementids_agedetermination['measurementid'].str.split(' ', n = 1, expand = True)\n",
    "            self.__measurementids_agedetermination = self.__measurementids_agedetermination[['measurementid','coreid','compositedepth']]\n",
    "            self.__measurementids_agedetermination['compositedepth'].replace(regex=True,inplace=True,to_replace=(r'_duplicate'+r'\\d'),value=r'')\n",
    "            self.__measurementids_agedetermination = self.__measurementids_agedetermination.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float}).drop_duplicates()\n",
    "            try:\n",
    "                __con = __engine.connect()\n",
    "                self.__down_measurement_agedetermination = pd.read_sql('measurement', __con)\n",
    "                self.__down_measurement_agedetermination = self.__down_measurement_ele.astype(dtype = {'measurementid':str, 'coreid': str, 'compositedepth': float})\n",
    "                self.__measurement_agedetermination_duplicate_check = self.__measurementids_agedetermination.append(self.__down_measurement_agedetermination)\n",
    "                self.__measurement_agedetermination_duplicates = self.__measurement_agedetermination_duplicate_check[self.__measurement_agedetermination_duplicate_check.duplicated() == True].reset_index(drop=True)\n",
    "                self.__measurement_agedetermination_duplicate_free = self.__measurementids_agedetermination.append(self.__measurement_agedetermination_duplicates)\n",
    "                self.__measurement_agedetermination_duplicate_free = self.__measurement_agedetermination_duplicate_free.drop_duplicates(keep = False)\n",
    "                self.__measurement_agedetermination_duplicate_free.to_sql('measurement', __con, if_exists='append', index = False)\n",
    "            except IntegrityError:\n",
    "                raise Exception(f'There is a problem with core {__coreid} for age determination proxy')\n",
    "            finally:\n",
    "                self.__input_agedetermination.to_sql('agedetermination', __con, if_exists='append', index = False)\n",
    "                __con.close()\n",
    "                print (f'I am done with core {__coreid} for age determination proxy')\n",
    "        except IntegrityError:   \n",
    "            print (f'I had an integrity error for {__coreid} - It seemes that age determination data were already uploaded.')\n",
    "            \n",
    "    \n",
    "    def run_data_upload(self):\n",
    "        self.__upload_scientist__()\n",
    "        self.__upload_expedition__()\n",
    "        self.__upload_lake__()\n",
    "        self.__upload_drilling__()\n",
    "        if self.__upload_stop == False:\n",
    "            self.__upload_participant__()\n",
    "            try: self.__upload_organic__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_grainsize__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_element__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_mineral__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_diatom__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_chironomid__()\n",
    "            except AttributeError: pass\n",
    "            try: self.__upload_pollen__()\n",
    "            except AttributeError: pass\n",
    "            self.__upload_age__()\n",
    "        else:\n",
    "            return \n",
    "        # finally:\n",
    "        #  del con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import data_preparation as d_p\n",
    "###\n",
    "core = d_p.data_preparation(filename = 'E:\\ARCLAKES-STANDARDIZED\\PG2023_raw_data.xlsx', suppress_message = True)\n",
    "core.run_data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist(s) already exist!\n",
      "Expedition already exists!\n",
      "Lake already exists!\n",
      "Core PG2023 already exist - Do you still want to continue uploading the data? Y/N? y\n",
      "Ok!\n",
      "Participant(s) already registered!\n",
      "I had an integrity error for PG2023 - It seemes that organic data were already uploaded.\n",
      "I had an integrity error for PG2023 - It seemes that element data were already uploaded.\n",
      "I had an integrity error for PG2023 - It seemes that mineral data were already uploaded.\n",
      "I am done with core PG2023 for diatom proxy\n",
      "I am done with core PG2023 for age determination proxy\n"
     ]
    }
   ],
   "source": [
    "###Upload###\n",
    "upload = connection_db(core, db = 'MAYHEM-Example', password = 'BoBoBernini')#, force_upload = True)\n",
    "upload.run_data_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
